<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Welcome to Yanting's Data Science Blog</title><link href="https://emmating.github.io/" rel="alternate"></link><link href="https://emmating.github.io/feeds/all.atom.xml" rel="self"></link><id>https://emmating.github.io/</id><updated>2017-07-19T16:45:00-07:00</updated><entry><title>How to Deal with High Cardinality Categorical Variables</title><link href="https://emmating.github.io/how-to-deal-with-high-cardinality-categorical-variables.html" rel="alternate"></link><published>2017-07-19T16:45:00-07:00</published><updated>2017-07-19T16:45:00-07:00</updated><author><name>Yanting Cao</name></author><id>tag:emmating.github.io,2017-07-19:/how-to-deal-with-high-cardinality-categorical-variables.html</id><summary type="html">&lt;h3&gt;Background and Methods&lt;/h3&gt;
&lt;p&gt;In machine learning problems, we encounter categorical features very often, such as gender, address, zip code, etc. 
For low cardinality attributes, which only takes a small number of possible values, one hot encoding (OHE) is widely used.
This encoding scheme represents each value of the original categorical …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Background and Methods&lt;/h3&gt;
&lt;p&gt;In machine learning problems, we encounter categorical features very often, such as gender, address, zip code, etc. 
For low cardinality attributes, which only takes a small number of possible values, one hot encoding (OHE) is widely used.
This encoding scheme represents each value of the original categorical with a binary vector with the i-th component 
set to one, and the rest set to zero. However, when having a high cardinality categorical feature with many unique values, 
OHE will give an extremely large sparse matrix, making it hard for application. &lt;/p&gt;
&lt;p&gt;The most frequently used method for dealing with high cardinality attributes is clustering. The basic idea is to reduce 
the N different sets of values to K different sets of values, where K &amp;lt; N. There are several value grouping schemes, 
including grouping values that exhibit similar target statistics (hierarchical clustering), or to use 
information-theoretical metric to merge each possible pair of clusters. Another methodis to use 
Principle Component Analysis (PCA) to reduce categorical data to a numerical representation. &lt;/p&gt;
&lt;h3&gt;Categorical Encoding Using Target Statistics&lt;/h3&gt;
&lt;p&gt;The basic idea is to map individual values of a high-cardinality categorical attribute to an estimate of the probability
or expected values of the dependent attribute. Let's take the 'building_id' attribute from the Two Sigma Connect Rental Listing
Competition as an example. The 'building_id' attribute is a high cardinality attribute having 7585 unique values. The target 
variable takes 3 values: high, medium, and low. We map the individual values X&lt;sub&gt;i&lt;/sub&gt; to a scaler S&lt;sub&gt;i&lt;/sub&gt;, 
representing an estimate of the probability of Y = c given X = X&lt;sub&gt;i&lt;/sub&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="images/tsc_high_cardinality.png"&gt;&lt;/p&gt;
&lt;p&gt;Be careful of data leakage if you use cross validation. &lt;/p&gt;
&lt;p&gt;In the Two Sigma Connect Competition, by implementing this method on three features, 'building_id', 'manager_id',
and 'display_address', and trained by a single xgboost model, the predict error measured by logloss improved 
from 0.549 to 0.539, a jump of 300 rankings in the competition leaderboard.  &lt;/p&gt;</content></entry><entry><title>Identifying Duplicate Quora Question Pairs (Kaggle Competition Bronze Medal Winner)</title><link href="https://emmating.github.io/identifying-duplicate-quora-question-pairs-kaggle-competition-bronze-medal-winner.html" rel="alternate"></link><published>2017-07-16T17:00:00-07:00</published><updated>2017-07-16T17:00:00-07:00</updated><author><name>Yanting Cao</name></author><id>tag:emmating.github.io,2017-07-16:/identifying-duplicate-quora-question-pairs-kaggle-competition-bronze-medal-winner.html</id><summary type="html">&lt;ul&gt;
&lt;li&gt;We explored the current methods in NLP, including word2vec embedding (gensim package in python), LSTMs(use keras neural networks API), tf-idf, python nltk package, etc. &lt;/li&gt;
&lt;li&gt;We built machine learning models which identified duplicate Quora question pairs with high accuracy (logloss ~0.151)&lt;/li&gt;
&lt;li&gt;We are ranked top 8% in this Kaggle …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;ul&gt;
&lt;li&gt;We explored the current methods in NLP, including word2vec embedding (gensim package in python), LSTMs(use keras neural networks API), tf-idf, python nltk package, etc. &lt;/li&gt;
&lt;li&gt;We built machine learning models which identified duplicate Quora question pairs with high accuracy (logloss ~0.151)&lt;/li&gt;
&lt;li&gt;We are ranked top 8% in this Kaggle competition among 3307 teams who participated, and got a Bronze Medal.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="## Problem Description"&gt;Problem Description&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[Methods Overview] (## Methods Overview)&lt;/li&gt;
&lt;li&gt;&lt;a href="## EDA of Quora Dataset"&gt;EDA of Quora Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="## Feature Engineering"&gt;Feature Engineering&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="### Basic Features"&gt;Basic Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="### NLTK Features"&gt;NLTK Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="### Word2vec features"&gt;Word2vec features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="### TF-IDF features"&gt;TF-IDF features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="### LSTMs features"&gt;LSTMs features&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="## Machine Learning Models"&gt;Machine Learning Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="## Model Ensemble"&gt;Model Ensemble&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Problem Description - Identifying Duplicate Questions&lt;/h2&gt;
&lt;p&gt;Quora is a question-and-answer online platform where questions are asked, answered, edited and organized 
by its community of users. Very often, people ask differently worded questions but with the same meaning. Multiple questions
with the same intent cause the seekers more time to find the best answers to their questions, and also cause the writers feel
they need to answer multiple versions of the same question. &lt;strong&gt;Identifying duplicate questions&lt;/strong&gt; will provide better experience
for both the users and writers. &lt;/p&gt;
&lt;p&gt;For example, the following question pairs are duplicates:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;question 1: How do I read and find my YouTube comments? vs question 2: How can I see all my Youtube comments?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;question 3: What are some examples of products that can be make from crude oil? vs question 4: What are some of the products made from crude oil?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The following question pairs are not duplicates:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;question 5: What is the step by step guide to invest in share market? vs question 6: What is the step by step guide to invest in share market in india?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;question 7: What's causing someone to be jealous? vs question 8: What can I do to avoid being jealous of someone?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Methods Overview:&lt;/h2&gt;
&lt;p&gt;To identify duplicate questions, we extracted features from text data including basic NLP, word2vec embedding, TF-IDF, LSTMs. 
We trained the features with Random Forest model, xgboost model, logistic regression and neural networks. 
Finally, we ensembled 12 models to make the final model more robust and to improve accuracy. &lt;/p&gt;
&lt;h2&gt;EDA of Quora Dataset&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://www.kaggle.com/c/quora-question-pairs/data"&gt;Quora dataset&lt;/a&gt; provided by Kaggle contains a train dataset and a test dataset. The train dataset consists 404290 question
pairs with each pairs labeled as 1(duplicate) or 0 (not duplicate). The test dataset consists 2345796 question pairs without 
labels, which is 5.8 fold as much as the train dataset. Evaluations are based on logloss between the predicted value and ground
truth. &lt;/p&gt;
&lt;p&gt;Let's take a look at the dataset. &lt;/p&gt;
&lt;p&gt;Data fields: id - id of the question pairs; qid1 - id of question 1; qid2 - id of question 2; question1 - full text of question 1;
question2 - full text of question 2&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="images/train_head.png"&gt;&lt;/p&gt;
&lt;p&gt;Data distribution by label&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="images/data_distribution_by_target_variable.png"&gt;&lt;/p&gt;
&lt;p&gt;We did Exploratory Data Analysis (EDA) and found the following features between train and test dataset. The values in train 
and test dataset are very close.  &lt;/p&gt;
&lt;p&gt;number of words:
- Median in train: 10.0 test: 10.0
- Average in train: 11.06 test: 11.02
- Maximum in train: 237 test: 238
- Minimum in train: 1 test: 1&lt;/p&gt;
&lt;p&gt;number of characters:
- Median in train: 51.0 test: 53.0
- Average in train: 59.82 test: 60.07
- Maximum in train: 1169 test: 1176
- Minimum in train: 1 test: 1&lt;/p&gt;
&lt;p&gt;We found that the number of common words between question pairs may be a good feature in building our prediction model. &lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="images/common_unigram_counts.png"&gt;&lt;/p&gt;
&lt;h2&gt;Feature Engineering&lt;/h2&gt;
&lt;p&gt;Feature engineering is a major part in building good machine learning models. We applied various methods to
extract features from text data. We used the following feature engineering methods, including basic features, NLP
features, word2vec features, TF-IDF transformation features, LSTM features as well as leaky features. I will cover
each methods as follows. &lt;/p&gt;
&lt;h3&gt;Basic Features&lt;/h3&gt;
&lt;p&gt;We crafted the following 18 basic features:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Feature&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;len_q&lt;/td&gt;
&lt;td&gt;length of characters in question inclduing whitespaces&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;len_char_q&lt;/td&gt;
&lt;td&gt;number of characters in question without whitespaces&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;diff_len&lt;/td&gt;
&lt;td&gt;difference in character length between question pairs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;char_diff_unq_stop&lt;/td&gt;
&lt;td&gt;difference in number of characters between question pairs after filtering stop words&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;char_ratio&lt;/td&gt;
&lt;td&gt;ratio of character length between question pairs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;len_word_q&lt;/td&gt;
&lt;td&gt;number of words in question&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wc_diff&lt;/td&gt;
&lt;td&gt;difference in number of words between question pairs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wc_diff_unique&lt;/td&gt;
&lt;td&gt;difference in number of unique words between question paris&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wc_diff_unq_stop&lt;/td&gt;
&lt;td&gt;difference in number of unique words between question pairs after filtering stop words&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wc_ratio&lt;/td&gt;
&lt;td&gt;ratio of word length between question pairs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wc_unique_ratio&lt;/td&gt;
&lt;td&gt;ratio of unique word length between question pairs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wc_ratio_unique_stop&lt;/td&gt;
&lt;td&gt;ratio of unique word length between q1 and q2 after filtering stop words&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;total_unique_words&lt;/td&gt;
&lt;td&gt;number of unique words in each question pair&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;total_unique_words_w_stop&lt;/td&gt;
&lt;td&gt;number of unique words in each question pair after filtering stop words&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;common_words&lt;/td&gt;
&lt;td&gt;number of common words in each question pair&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;word_match&lt;/td&gt;
&lt;td&gt;number of common words in question pairs over total number of words in question pairs after filtering stop words&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2_SWC&lt;/td&gt;
&lt;td&gt;word match using 2-grams&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3_SWC&lt;/td&gt;
&lt;td&gt;word match using 3-grams&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1_SWC_w_stops&lt;/td&gt;
&lt;td&gt;word match after filtering stop words&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2_SWC_w_stops&lt;/td&gt;
&lt;td&gt;word match using 2-grams after filtering stop words&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3_SWC_w_stops&lt;/td&gt;
&lt;td&gt;word match using 3-grams after filtering stop words&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Jaccard&lt;/td&gt;
&lt;td&gt;the shared words count over total words count in each question pair&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;same_start&lt;/td&gt;
&lt;td&gt;return 1 if a question pair has same start word, otherwise return 0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;NLTK Features&lt;/h3&gt;
&lt;p&gt;We used nltk package in Python, and crafted the following features. &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Feature&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;WC_NN&lt;/td&gt;
&lt;td&gt;number of common nouns in each question pair&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;WC_CD&lt;/td&gt;
&lt;td&gt;number of common numbers in each question pair&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;WC_VB&lt;/td&gt;
&lt;td&gt;number of common verbs in each question pair&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;WC_JJ&lt;/td&gt;
&lt;td&gt;number of common adjective in each question pair&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;nonlatin_shared&lt;/td&gt;
&lt;td&gt;number of common nonlatin characters in each question pair&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;havewhat&lt;/td&gt;
&lt;td&gt;return 1 if a question contains what, otherwise return 0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;havewhen&lt;/td&gt;
&lt;td&gt;return 1 if a question contains when, otherwise return 0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;havewho&lt;/td&gt;
&lt;td&gt;return 1 if a question contains who, otherwise return 0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;havewhy&lt;/td&gt;
&lt;td&gt;return 1 if a question contains why, otherwise return 0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;havehow&lt;/td&gt;
&lt;td&gt;return 1 if a question contains how, otherwise return 0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;nonascii&lt;/td&gt;
&lt;td&gt;return 1 if a question contains nonascii characters, otherwise return 0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;nonlatin&lt;/td&gt;
&lt;td&gt;return 1 if a question contains nonlatin characters, otherwise return 0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Word2vec features&lt;/h3&gt;
&lt;p&gt;Word2vec is a two-layer neural network model that is used to produce word embeddings. Word2vec takes a large corpus of text as input
and produces a vector space of several hundreds dimensions. Words with similar meanings are close in distance in their
vector space. We imported the &lt;a href="https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"&gt;Google pretrained word2vec model&lt;/a&gt;, and run the model using the gensim package in python. It 
outputs a vector for each word in our data. Then, we computed the distance features which measures the similarity 
between vectors, the skewness and kurtosis features which measure the shape of the distribution. &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Feature&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1_ND&lt;/td&gt;
&lt;td&gt;normalized word mover distance&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2_ND&lt;/td&gt;
&lt;td&gt;normalized word mover distance using 2-grams&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3_ND&lt;/td&gt;
&lt;td&gt;normalized word mover distance using 3-grams&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1_ND_w_stops&lt;/td&gt;
&lt;td&gt;normalized word mover distance after filtering stop words&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2_ND_w_stops&lt;/td&gt;
&lt;td&gt;normalized word mover distance using 2-grams after filtering stop words&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3_ND_w_stops&lt;/td&gt;
&lt;td&gt;normalized word mover distance using 3-grams after filtering stop words&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;cosine_distance&lt;/td&gt;
&lt;td&gt;cosine distance&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;cityblock_distance&lt;/td&gt;
&lt;td&gt;cityblock distance&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;jaccard_distance&lt;/td&gt;
&lt;td&gt;jaccard distance&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;canberra_distance&lt;/td&gt;
&lt;td&gt;canberra distance&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;euclidean_distance&lt;/td&gt;
&lt;td&gt;euclidean distance&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;minkowski_distance&lt;/td&gt;
&lt;td&gt;minkowski distance&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;braycurtis_distance&lt;/td&gt;
&lt;td&gt;braycurtis distance&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;skew_q1vec&lt;/td&gt;
&lt;td&gt;skewness of q1 vector&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;skew_q2vec&lt;/td&gt;
&lt;td&gt;skewness of q1 vector&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kur_q1vec&lt;/td&gt;
&lt;td&gt;kurtosis of q1 vector&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kur_q2vec&lt;/td&gt;
&lt;td&gt;kurtosis of q2 vector&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;TF-IDF features&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf"&gt;tf-idf&lt;/a&gt;, short for term frequency-inverse document frequency, is a numerical statistic that measures the importance 
of a word in a sentence. The importance is denoted by term frequency in a sentence (tf), and offset by the frequency of 
the word in corpus(idf). &lt;/p&gt;
&lt;p&gt;![alt text][tfidf]&lt;/p&gt;
&lt;p&gt;For example, in the sentence 'How do I read and find my YouTube comments?', 'I' has the same term frequency as 'Youtube', 
but 'I' has a higher frequency than 'Youtube' in the corpus, so the high frequency of 'I' 
in this sentence is offset by the high frequency of 'I' in corpus, so 'I' is not important. On the other hand,
'Youtube' has a high frequency in this sentence and low frequency in the corpus, so 'Youtube' is still considered as 
important. Therefore, 'Youtube' has better prediction power than 'I'. In other words, the more rare a term is, the larger
idf. &lt;/p&gt;
&lt;p&gt;After tfidf transformation, we crafted the following features&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Feature&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;tfidf_wm&lt;/td&gt;
&lt;td&gt;word match after tfidf&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tfidf_wm_stops&lt;/td&gt;
&lt;td&gt;word match after tfidf and filtering stop words&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2_SWC_IDF&lt;/td&gt;
&lt;td&gt;word match using 2-grams after tfidf&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3_SWC_IDF&lt;/td&gt;
&lt;td&gt;word match using 3-grams after tfidf&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2_SWC_IDF_w_stops word match using 2-grams after tfidf and filtering stop words&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3_SWC_IDF_w_stops word match using 3-grams after tfidf and filtering stop words&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1_ND_IDF&lt;/td&gt;
&lt;td&gt;normal distance after tfidf&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2_ND_IDF&lt;/td&gt;
&lt;td&gt;normal distance using 2-grams after tfidf&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3_ND_IDF&lt;/td&gt;
&lt;td&gt;normal distance using 3-grams after tfidf&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1_ND_IDF_w_stops&lt;/td&gt;
&lt;td&gt;normal distance after tfidf and filtering stop words&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2_ND_IDF&lt;/td&gt;
&lt;td&gt;normal distance using 2-grams after tfidf and filtering stop words&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3_ND_IDF&lt;/td&gt;
&lt;td&gt;normal distance using 3-grams after tfidf and filtering stop words&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;LSTMs (Long Short Term Memory networks) features&lt;/h3&gt;
&lt;p&gt;LSTMs is a special kind of recurrent neural network (RNN) which works very well in predicting sequential patterns such
as text, speech, audio, video, physical processes, time series(sensor) data, anomaly detection, etc. 
The details of LSTMs are well explained in the &lt;a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;blog post&lt;/a&gt; by Christopher Olah, and &lt;a href="https://brohrer.github.io/blog.html"&gt;the blog post&lt;/a&gt; by Brandon Rohrer.&lt;/p&gt;
&lt;p&gt;We converted each word in our dataset to an unique integer identifier after data clean and preprocessing.
By Keras default embedding, it was converted to a embedding matrix. Feeding the embeding matrix to LSTMs, we obtained the
output from 32 neurons which gives us 32 LSTM features. &lt;/p&gt;
&lt;h3&gt;Leak features&lt;/h3&gt;
&lt;p&gt;Leak features are playing an important role in this competition. It is useful in competition but not practical in real 
world projects because we do not know the true target value for test dataset. &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Feature&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;q1_frequency&lt;/td&gt;
&lt;td&gt;the number of times question1 appearance in the dataset&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;q2_frequency&lt;/td&gt;
&lt;td&gt;the number of times question2 appearance in the dataset&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;q1_q2_intersect&lt;/td&gt;
&lt;td&gt;the number of shared questions question1 and question2 all formed question pairs with in the dataset&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Machine Learning Models&lt;/h2&gt;
&lt;p&gt;We have built models including xgboost, random forest, logistic regression, neural network, and support vector machines. 
Using different subsets of data, we built a total of 12 models. The best single model is a xgboost model with 83 features,
it gives 0.15239 logloss on the public leaderboard.&lt;/p&gt;
&lt;h2&gt;Model Stacking&lt;/h2&gt;
&lt;p&gt;There are several ways to ensemble models, the most widely used methods include bagging, boosting and stacking. Here,
we use stacking. The basic idea of stacking is to build different models which output intermediate prediction, also 
called meta features. Those meta features are combined and fed into a new model to predict target.&lt;br&gt;
&lt;img alt="alt text" src="images/stacking.png"&gt;&lt;/p&gt;
&lt;p&gt;We stacked 12 models. Logloss reaches 0.15146 on the public leaderboard, which is almost 0.001 improvement on the best 
single model.   &lt;/p&gt;
&lt;p&gt;It was so much fun and a great learning experience working on the Quora project with my talented team members. Thank you !&lt;/p&gt;</content><category term="NLP"></category><category term="Neural Networks"></category><category term="LSTMs"></category><category term="tfidf"></category><category term="Word2vec"></category><category term="Gradient Boosting"></category><category term="Random Forest"></category><category term="Stacking"></category><category term="Kaggle"></category><category term="Python"></category></entry><entry><title>Using Geopy for Spatial Data Analysis</title><link href="https://emmating.github.io/using-geopy-for-spatial-data-analysis.html" rel="alternate"></link><published>2017-06-23T16:45:00-07:00</published><updated>2017-06-23T16:45:00-07:00</updated><author><name>Yanting Cao</name></author><id>tag:emmating.github.io,2017-06-23:/using-geopy-for-spatial-data-analysis.html</id><summary type="html">&lt;p&gt;&lt;a href="https://geopy.readthedocs.io/en/1.10.0/"&gt;Geopy&lt;/a&gt; is a useful python package to deal with spatial data, such as locating the coordinates of addresses, cities, countries, 
and landmarks or reverse. In the &lt;a href="https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries"&gt;Two Sigma Connect Competition&lt;/a&gt; in &lt;a href="https://www.kaggle.com/"&gt;Kaggle&lt;/a&gt;, I found that some of the given latitude longitude
do not match the addresses, so I use Geopy …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://geopy.readthedocs.io/en/1.10.0/"&gt;Geopy&lt;/a&gt; is a useful python package to deal with spatial data, such as locating the coordinates of addresses, cities, countries, 
and landmarks or reverse. In the &lt;a href="https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries"&gt;Two Sigma Connect Competition&lt;/a&gt; in &lt;a href="https://www.kaggle.com/"&gt;Kaggle&lt;/a&gt;, I found that some of the given latitude longitude
do not match the addresses, so I use Geopy to clean the mismatched latitude and longitude of rental listings. &lt;/p&gt;
&lt;h3&gt;Import Packages and Read Data&lt;/h3&gt;
&lt;p&gt;I am working with rental listings in New York City, the addresses should be in NYC, so I append 'New York City' to the addresses
ensure its accuracy. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;geopy.geocoders&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GoogleV3&lt;/span&gt;
&lt;span class="n"&gt;bad_location&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;street_address&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bad_location&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;street_address&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;,New York City&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Geoencoding&lt;/h3&gt;
&lt;p&gt;There are several geolocation services we can use, such as Google Maps, Bing Maps, or Yahoo Boss. I choose to use Google Maps 
Geoencoding API (GoogleV3) because it is insensitive to missing information. Using a standard GoogleV3 API, we can have 2,500
free requests per day and 50 requests per second. Before using it, we need to register to get a free API key following &lt;a href="https://developers.google.com/maps/documentation/geocoding/get-api-key"&gt;this&lt;/a&gt;
link.   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def do_geocode(address):
    geolocator = GoogleV3(api_key = &amp;#39;your_api_key&amp;#39;)
    try:
        location = address.apply(geolocator.geocode)
        latitude = location.apply(lambda x: x.latitude)
        longitude = location.apply(lambda x: x.longitude)
        return (latitude, longitude)
    except GeocoderTimedOut:
        return do_geocode()

(latitude, longitude) = do_geocode(bad_location[&amp;#39;street_address&amp;#39;])
bad_location.loc[:,&amp;#39;latitude&amp;#39;] = latitude
bad_location.loc[:,&amp;#39;longitude&amp;#39;] = longitude
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Quite simple, within a few minutes, we can generate the geolocation for the addresses. &lt;/p&gt;</content><category term="Python"></category><category term="Geopy"></category><category term="Spatial Data Analysis"></category><category term="GoogleV3"></category><category term="Geoencode"></category></entry><entry><title>Creating Word Cloud in Python</title><link href="https://emmating.github.io/creating-word-cloud-in-python.html" rel="alternate"></link><published>2017-06-22T11:45:00-07:00</published><updated>2017-06-22T11:45:00-07:00</updated><author><name>Yanting Cao</name></author><id>tag:emmating.github.io,2017-06-22:/creating-word-cloud-in-python.html</id><summary type="html">&lt;p&gt;In text analysis, creating word clouds is a useful technique to visualize text data. Words bigger and bolder in size represent 
a higher frequency of occurance in word corpus. In other word, key words stand out and catch our eyes. The color of the text are 
generated randomly. &lt;/p&gt;
&lt;p&gt;It is …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In text analysis, creating word clouds is a useful technique to visualize text data. Words bigger and bolder in size represent 
a higher frequency of occurance in word corpus. In other word, key words stand out and catch our eyes. The color of the text are 
generated randomly. &lt;/p&gt;
&lt;p&gt;It is very easy to create word cloud in Python using wordcloud package. I will use word cloud to visualize features and display address 
of rental listings from the &lt;a href="https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries"&gt;Two Sigma Connect Competition&lt;/a&gt; in the recent Kaggle Competition. &lt;/p&gt;
&lt;h3&gt;Import Packages and Read Data&lt;/h3&gt;
&lt;p&gt;Let's get started by importing the following packages and read the data&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;wordcloud&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;WordCloud&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="n"&gt;train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;../Data/train.json&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Preprocess text data&lt;/h3&gt;
&lt;p&gt;Next, extract text data from 'feature' and 'display_address' columns in the dataframe&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;text = &amp;#39;&amp;#39;
text_dispadd = &amp;#39;&amp;#39;
for ind, row in train.iterrows():
    for feature in row[&amp;#39;features&amp;#39;]:
        text = &amp;quot; &amp;quot;.join([text, &amp;quot;_&amp;quot;.join(feature.strip().split(&amp;quot; &amp;quot;))])
    text_dispadd = &amp;quot; &amp;quot;.join([text_dispadd,&amp;quot;_&amp;quot;.join(row[&amp;#39;display_address&amp;#39;].strip().split(&amp;quot; &amp;quot;))])
text = text.strip()
text_dispadd = text_dispadd.strip()
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Create word clouds&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;plt.figure(figsize=(12,6))
wc = WordCloud(background_color=&amp;#39;white&amp;#39;, 
            width=600, 
            height=300,
            max_font_size=50, 
            max_words=40).generate(text)
plt.imshow(wc)
plt.title(&amp;quot;Features&amp;quot;, fontsize=20)
plt.axis(&amp;quot;off&amp;quot;)
plt.show()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Thus we created the word cloud for rental listing features.
We get a general sense of words used to describe features of rental listings in our training set.
&lt;img alt="alt text" src="images/feature_word_cloud.png"&gt;&lt;/p&gt;
&lt;p&gt;We can change the background color of word cloud by setting &lt;code&gt;background_color = 'black'&lt;/code&gt; for display_address column. 
There are some addresses that appear more frequent in our training set. 
&lt;img alt="alt text" src="images/display_address_word_cloud.png"&gt;&lt;/p&gt;</content><category term="Python"></category><category term="Word Clouds"></category><category term="Text Analysis"></category></entry><entry><title>How I Build My First Pelican Blog</title><link href="https://emmating.github.io/how-i-build-my-first-pelican-blog.html" rel="alternate"></link><published>2017-06-14T16:45:00-07:00</published><updated>2017-06-14T16:45:00-07:00</updated><author><name>Yanting Cao</name></author><id>tag:emmating.github.io,2017-06-14:/how-i-build-my-first-pelican-blog.html</id><summary type="html">&lt;p&gt;After completed several data science projects, I am eager to document them and share them with people. 
It took me several days to research, set up and write my blog, but I feel it can be much easier and faster 
to build a Pelican blog, so I am sharing with …&lt;/p&gt;</summary><content type="html">&lt;p&gt;After completed several data science projects, I am eager to document them and share them with people. 
It took me several days to research, set up and write my blog, but I feel it can be much easier and faster 
to build a Pelican blog, so I am sharing with you the lessons I have learned in building my first data science blog,
the code is available &lt;a href="https://github.com/emmating/emmating.github.io"&gt;here&lt;/a&gt;.  &lt;/p&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="### Find a Static Site Generator"&gt;Find a Static Site Generator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="### Basic Setup"&gt;Basic Setup&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#### Install and Activate Virtual Environment"&gt;Install and Activate Virtual Environment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#### Install Pelican under Virtual Environment"&gt;Install Pelican under Virtual Environment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#### Quickstart Your Site"&gt;Quickstart Your Site&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="### Install Pelican Theme"&gt;Install Pelican Theme&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="### Write Content"&gt;Write Content&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="### Publish Your Site"&gt;Publish Your Site&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="### More Setup"&gt;More Setups&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#### Side Bar"&gt;Side Bar&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#### Banner"&gt;Banner&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Find a Static Site Generator&lt;/h3&gt;
&lt;p&gt;There are lots of discussions on the Internet about pros and cons between static site and dynamic site &lt;a href="http://www.codeconquest.com/website/static-vs-dynamic-websites/"&gt;static_vs_dynamic&lt;/a&gt;. For me, 
I am build a small size personal website, static site will fully meet my needs, and it is faster to load, and it allows 
me to write blog posts in simple format, such as markdown file. So if you are write a small/medium personal site, go with 
static! There are several static site generators to choose from, such as jekyll, nikola, etc. Recently, Pelicanis has gain lots of 
popularity among the data science community, and I saw many people are moving their blogs to Pelican, so I choose to use Pelican, 
it has lots of online resources and user community support, and it is built in my favorite Python language. &lt;/p&gt;
&lt;h3&gt;Basic Setup&lt;/h3&gt;
&lt;p&gt;Read official &lt;a href="http://docs.getpelican.com/en/stable/"&gt;Pelican documentation&lt;/a&gt; first, most of the questions can be found there. &lt;/p&gt;
&lt;h4&gt;Install and Activate Virtual Environment&lt;/h4&gt;
&lt;p&gt;A virtual environment is an isolated Python environment in which we store our Pelican blog site. We build an isolated environment 
so that if we make changes or update a Python package or Python version outside the virtual environment, it does not affect the Python
used in our Pelican site. 
Steps are as follows: 
- open your Linux terminal(I use git bash), install virtualenv, and activate virtualenv, after activation, you will see your Linux command
start with(pelican)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip install virtualenv
virtualenv ~/virtualenvs/pelican
cd ~/virtualenvs
source ~/virtualenvs/pelican/Scripts/activate
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Install Pelican Under Virtual Environment&lt;/h4&gt;
&lt;p&gt;Next, we install Pelican under virtualenv, make sure (pelican) is shown at the beginning of your Linux command.
The last four commands are optional, use them if you want to install Markdown, typogrify, update pelican, or read help file.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip install pelican
pip install Markdown
pip install typogrify
pip install –upgrade pelican
pelican –help
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Quickstart Your Site&lt;/h4&gt;
&lt;p&gt;Quickstart your site, select your choices. 
After the following command, you will see the output of your project location, &lt;strong&gt;write down the location&lt;/strong&gt;, you will use it later. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pelican-quickstart
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Install Pelican Theme&lt;/h3&gt;
&lt;p&gt;Pelican comes with its default theme, but you can change your themes really easy from &lt;a href="https://github.com/getpelican/pelican-themes"&gt;Pelican-themes github repo&lt;/a&gt;. Before selecting the
theme you want to install, take a look at the preview of various &lt;a href="http://www.pelicanthemes.com/"&gt;Pelican themes&lt;/a&gt;. I use the very popular &lt;a href="https://github.com/getpelican/pelican-themes/tree/master/pelican-bootstrap3"&gt;bootstrap3 theme&lt;/a&gt;, which comes with many 
functions. I choose bootstrap3 theme particularly because it support IPython Notebook, it includes many more themes based on bootstrap3 which
you can choose from &lt;a href="http://bootswatch.com/"&gt;bootswatch&lt;/a&gt;. Moreover, you can include a profile picture and a simple description of yourself.&lt;/p&gt;
&lt;p&gt;First, download or clone from the github repo containing the theme you want to use, next, install it to your Pelican blog by the following commmand
The last two commands are optional which list themes installed.  &lt;strong&gt;write down the location where it is stored&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pelican-themes --install ~/virtualenvs/pelican-themes/boostrap3 --verbose
pelican-themes –l # list themes installed
pelican-themes –v –l # list themes installed with path
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For a boostrap3 theme to work, you also need to install pelican-plugins from &lt;a href="https://github.com/getpelican/pelican-plugins/tree/master/tag_cloud"&gt;this&lt;/a&gt; github repo. Clone or copy the pelican-plugin folder, 
add the following lines to pelicanconf.py located in ~/virtualenv/pelican folder. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;THEME = &amp;#39;path/to/bootstrap3/stored/pelican-bootstrap3&amp;#39;
JINJA_ENVIRONMENT = {&amp;#39;extensions&amp;#39;: [&amp;#39;jinja2.ext.i18n&amp;#39;]}
PLUGIN_PATHS = [&amp;#39;/path/to/git/pelican-plugins&amp;#39;]
PLUGINS = [&amp;#39;i18n_subsites&amp;#39;]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;To select a theme from &lt;a href="http://bootswatch.com/"&gt;bootswatch&lt;/a&gt;: http://bootswatch.com/, all you need to do is add the following line to your pelicanconf.py
    BOOTSTRAP_THEME = 'bootswatch_theme'&lt;/p&gt;
&lt;h3&gt;Write Content&lt;/h3&gt;
&lt;p&gt;Create three folders in your content folder, name them images, pages and posts, respectively. images folder stores all the images you want to use in your blog.
Next, create a markdown file (about.md) and store in pages folder, where about_me is stored. In the end, create several markdown files in posts folder where each file contains 
the content of a post. &lt;/p&gt;
&lt;p&gt;Include the following metadata in each of your markdown files.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Title: How I Build My First Pelican Blog&lt;/p&gt;
&lt;p&gt;Date: 2017-06-14 16:00&lt;/p&gt;
&lt;p&gt;Modified: 2017-06-15 16:00 &lt;/p&gt;
&lt;p&gt;Category: Tutorials&lt;/p&gt;
&lt;p&gt;Tags: Python, Pelican&lt;/p&gt;
&lt;p&gt;Slug: my-super-post  &lt;/p&gt;
&lt;p&gt;Authors: Yanting Cao&lt;/p&gt;
&lt;p&gt;Summary: A tutorial of my first pelican blog&lt;/p&gt;
&lt;p&gt;Write my content here ...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you are not familar with markdown syntax, here is a &lt;a href="https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet"&gt;link to a Markdown cheatsheet&lt;/a&gt;. &lt;/p&gt;
&lt;h3&gt;Publish Your Site&lt;/h3&gt;
&lt;p&gt;We have gone this far, let's now take a look at our blog.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pelican ~/virtualenvs/pelican/content/ -s ~/virtualenvs/pelican/pelicanconf.py
cd output
python –m http.server
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Once the basic server has been started, you can view your site at http://localhost:8000/&lt;/p&gt;
&lt;h3&gt;More setups&lt;/h3&gt;
&lt;p&gt;If you want to make your blog look better, I will teach you more setups, including side bar and banner (this specifically applies to bootstrap3 theme,
if you use a different theme, refer to the descriptions in the theme's github repo)&lt;/p&gt;
&lt;h4&gt;Side Bar&lt;/h4&gt;
&lt;p&gt;Side bar has options including display an image, Social(github, linkedin, twitter, facebook, etc.), Categories, Tags and Links.&lt;/p&gt;
&lt;p&gt;Display an image on the side bar
Save image in png file under 'content/images', add the following lines in your pelicanconf.py&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;PATH = &amp;#39;content&amp;#39;
STATIC_PATHS = [&amp;#39;images&amp;#39;]
AVATAR = &amp;#39;images/side_bar_pic.png&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Show Social Links
Include the following lines in your pelicanconf.py&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;SOCIAL = ((&amp;#39;twitter&amp;#39;, &amp;#39;http://twitter.com/youraccount&amp;#39;, &amp;#39;twitter&amp;#39;),
      (&amp;#39;linkedin&amp;#39;, &amp;#39;http://www.linkedin.com/in/youraccount&amp;#39;, &amp;#39;linkedin&amp;#39;),
      (&amp;#39;github&amp;#39;, &amp;#39;http://github.com/youraccount&amp;#39;, &amp;#39;github&amp;#39;),
      (&amp;#39;stackoverflow&amp;#39;, &amp;#39;http://stackoverflow.com/users/youraccount&amp;#39;, &amp;#39;stack-overflow&amp;#39;)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Show Tags
Also in your pelicanconf.py, add the following codeconquest&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;DISPLAY_TAGS_ON_SIDEBAR = True
PLUGINS = [&amp;#39;tag_cloud&amp;#39;]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Show Categories
Add the following line to pelicanconf.py&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;DISPLAY_CATEGORIES_ON_SIDEBAR = True
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Show Links
Add the following line to pelicanconf.py    &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;LINKS = ((&amp;#39;Kaggle&amp;#39;,&amp;#39;kaggle.com&amp;#39;),
     (&amp;#39;Rady School of Management, UCSD&amp;#39;, &amp;#39;http://rady.ucsd.edu/&amp;#39;),
     (&amp;#39;Python.org&amp;#39;, &amp;#39;http://python.org/&amp;#39;),
     (&amp;#39;Pelican&amp;#39;, &amp;#39;http://getpelican.com/&amp;#39;))
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Banner&lt;/h4&gt;
&lt;p&gt;Adding a banner image and subtitle to your blog by the following code&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;BANNER = &amp;#39;/path/to/banner.png&amp;#39;
BANNER_SUBTITLE = &amp;#39;This is my subtitle&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, we have built our first Pelican blog. Many thanks to &lt;a href="http://docs.getpelican.com/en/stable/#"&gt;Pelican&lt;/a&gt;, the free static site generator, as well as to &lt;a href="http://dandydev.net/about"&gt;Daan Debie&lt;/a&gt;,
author of bootstrap3. Let's now start writing in our blogs ! &lt;/p&gt;</content><category term="Python"></category><category term="Pelican"></category></entry></feed>